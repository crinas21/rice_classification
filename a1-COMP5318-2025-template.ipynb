{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Rice Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group number: A1 group-set2 169\n",
    "##### Student 1 SID: 520463341\n",
    "##### Student 2 SID: ...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the rice dataset: rice-final2.csv\n",
    "raw_data = np.genfromtxt(\"rice-final2.csv\", delimiter=\",\", dtype=str, skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X_raw = raw_data[:, :-1]\n",
    "y_raw = raw_data[:, -1]\n",
    "\n",
    "# 1. Replace missing attributes\n",
    "X_raw[X_raw == '?'] = np.nan # Replace '?' with np.nan for imputation\n",
    "X = X_raw.astype(float)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# 2. Normalise using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "# 3. Change class values to 0 and 1\n",
    "y = np.where(y_raw == 'class1', 0, 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4628,0.5406,0.5113,0.4803,0.7380,0.4699,0.1196,1\n",
      "0.4900,0.5547,0.5266,0.5018,0.7319,0.4926,0.8030,1\n",
      "0.6109,0.6847,0.6707,0.5409,0.8032,0.6253,0.1185,0\n",
      "0.6466,0.6930,0.6677,0.5961,0.7601,0.6467,0.2669,0\n",
      "0.6712,0.6233,0.4755,0.8293,0.3721,0.6803,0.4211,1\n",
      "0.2634,0.2932,0.2414,0.4127,0.5521,0.2752,0.2825,1\n",
      "0.8175,0.9501,0.9515,0.5925,0.9245,0.8162,0.0000,0\n",
      "0.3174,0.3588,0.3601,0.3908,0.6921,0.3261,0.8510,1\n",
      "0.3130,0.3050,0.2150,0.5189,0.3974,0.3159,0.4570,1\n",
      "0.5120,0.5237,0.4409,0.6235,0.5460,0.5111,0.3155,1\n"
     ]
    }
   ],
   "source": [
    "# Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first ten rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: numpy of rows to print\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "\n",
    "        if example_num == len(X)-1:\n",
    "            print(y[example_num],end=\"\")\n",
    "        else:\n",
    "            print(y[example_num])\n",
    "\n",
    "print_data(X_scaled, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# The stratified folds from cvKFold should be provided to the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def logregClassifier(X, y):\n",
    "    model = LogisticRegression(random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naïve Bayes\n",
    "def nbClassifier(X, y):\n",
    "    model = GaussianNB()\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def dtClassifier(X, y):\n",
    "    model = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "    base_estimator = DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth, random_state=0)\n",
    "    model = BaggingClassifier(estimator=base_estimator,\n",
    "                              n_estimators=n_estimators,\n",
    "                              max_samples=max_samples,\n",
    "                              random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "    base_estimator = DecisionTreeClassifier(criterion=\"entropy\", max_depth=max_depth, random_state=0)\n",
    "    model = AdaBoostClassifier(estimator=base_estimator,\n",
    "                               n_estimators=n_estimators,\n",
    "                               learning_rate=learning_rate,\n",
    "                               random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold)\n",
    "    return scores.mean()\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    model = GradientBoostingClassifier(n_estimators=n_estimators,\n",
    "                                       learning_rate=learning_rate,\n",
    "                                       random_state=0)\n",
    "    scores = cross_val_score(model, X, y, cv=cvKFold)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR average cross-validation accuracy: 0.9386\n",
      "NB average cross-validation accuracy: 0.9264\n",
      "DT average cross-validation accuracy: 0.9179\n",
      "Bagging average cross-validation accuracy: 0.9414\n",
      "AdaBoost average cross-validation accuracy: 0.9407\n",
      "GB average cross-validation accuracy: 0.9321\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Part 1:\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 50\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 5\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 50\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 5\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 50\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "print(f\"LR average cross-validation accuracy: {logregClassifier(X_scaled, y):.4f}\")\n",
    "print(f\"NB average cross-validation accuracy: {nbClassifier(X_scaled, y):.4f}\")\n",
    "print(f\"DT average cross-validation accuracy: {dtClassifier(X_scaled, y):.4f}\")\n",
    "print(f\"Bagging average cross-validation accuracy: {bagDTClassifier(X_scaled, y, bag_n_estimators, bag_max_samples, bag_max_depth):.4f}\")\n",
    "print(f\"AdaBoost average cross-validation accuracy: {adaDTClassifier(X_scaled, y, ada_n_estimators, ada_learning_rate, ada_bag_max_depth):.4f}\")\n",
    "print(f\"GB average cross-validation accuracy: {gbClassifier(X_scaled, y, gb_n_estimators, gb_learning_rate):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7]\n",
    "p = [1, 2]\n",
    "\n",
    "\n",
    "def bestKNNClassifier(X, y):\n",
    "    param_grid = {\n",
    "        'n_neighbors': k,\n",
    "        'p': p\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    model = KNeighborsClassifier()\n",
    "    grid = GridSearchCV(model, param_grid, cv=cvKFold)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    best_params = grid.best_params_\n",
    "    cv_acc = grid.best_score_\n",
    "    test_acc = accuracy_score(y_test, best_model.predict(X_test))\n",
    "    \n",
    "    return best_params['n_neighbors'], best_params['p'], cv_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# You should use SVC from sklearn.svm with kernel set to 'rbf'\n",
    "C = [0.01, 0.1, 1, 5] \n",
    "gamma = [0.01, 0.1, 1, 10]\n",
    "\n",
    "def bestSVMClassifier(X, y):\n",
    "    param_grid = {\n",
    "        'C': C,\n",
    "        'gamma': gamma\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    model = SVC(kernel='rbf', random_state=0)\n",
    "    grid = GridSearchCV(model, param_grid, cv=cvKFold)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    best_params = grid.best_params_\n",
    "    cv_acc = grid.best_score_\n",
    "    test_acc = accuracy_score(y_test, best_model.predict(X_test))\n",
    "    \n",
    "    return best_params['C'], best_params['gamma'], cv_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‘sqrt’.\n",
    "n_estimators = [10, 30, 60, 100]\n",
    "max_leaf_nodes = [6, 12]\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    param_grid = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_leaf_nodes': max_leaf_nodes\n",
    "    }\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "    \n",
    "    model = RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=0)\n",
    "    grid = GridSearchCV(model, param_grid, cv=cvKFold)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    best_params = grid.best_params_\n",
    "    cv_acc = grid.best_score_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    weighted_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return best_params['n_estimators'], best_params['max_leaf_nodes'], cv_acc, test_acc, macro_f1, weighted_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best k: 5\n",
      "KNN best p: 1\n",
      "KNN cross-validation accuracy: 0.9371\n",
      "KNN test set accuracy: 0.9257\n",
      "\n",
      "SVM best C: 5.0000\n",
      "SVM best gamma: 1.0000\n",
      "SVM cross-validation accuracy: 0.9457\n",
      "SVM test set accuracy: 0.9343\n",
      "\n",
      "RF best n_estimators: 30\n",
      "RF best max_leaf_nodes: 12\n",
      "RF cross-validation accuracy: 0.9390\n",
      "RF test set accuracy: 0.9371\n",
      "RF test set macro average F1: 0.9355\n",
      "RF test set weighted average F1: 0.9370\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid Search with 10-fold stratified cross-validation (GridSearchCV in sklearn). \n",
    "# The stratified folds from cvKFold should be provided to GridSearchV\n",
    "\n",
    "# This should include using train_test_split from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. All results should be printed to 4 decimal places except for\n",
    "# \"k\", \"p\", n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "\n",
    "k_best, p_best, knn_cv_acc, knn_test_acc = bestKNNClassifier(X_scaled, y)\n",
    "print(f\"KNN best k: {k_best}\")\n",
    "print(f\"KNN best p: {p_best}\")\n",
    "print(f\"KNN cross-validation accuracy: {knn_cv_acc:.4f}\")\n",
    "print(f\"KNN test set accuracy: {knn_test_acc:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "C_best, gamma_best, svm_cv_acc, svm_test_acc = bestSVMClassifier(X_scaled, y)\n",
    "print(f\"SVM best C: {C_best:.4f}\")\n",
    "print(f\"SVM best gamma: {gamma_best:.4f}\")\n",
    "print(f\"SVM cross-validation accuracy: {svm_cv_acc:.4f}\")\n",
    "print(f\"SVM test set accuracy: {svm_test_acc:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "n_best, leaf_best, rf_cv_acc, rf_test_acc, macro_f1, weighted_f1 = bestRFClassifier(X_scaled, y)\n",
    "print(f\"RF best n_estimators: {n_best}\")\n",
    "print(f\"RF best max_leaf_nodes: {leaf_best}\")\n",
    "print(f\"RF cross-validation accuracy: {rf_cv_acc:.4f}\")\n",
    "print(f\"RF test set accuracy: {rf_test_acc:.4f}\")\n",
    "print(f\"RF test set macro average F1: {macro_f1:.4f}\")\n",
    "print(f\"RF test set weighted average F1: {weighted_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write one paragraph describing the most important thing that you have learned throughout this assignment.\n",
    "##### Student 1: The most important thing I learned throughout this assignment was how to actually apply and compare different machine learning classifiers in practice in a real-world context and using real data. While I had previously understood the theory behind various models, this was the first time I had to implement them side by side, evaluate their performance fairly using cross-validation, and interpret the results on a real dataset. It taught me not just how each model works in isolation, but how they behave relative to each other under the same conditions. This helped me understand their relative strengths. I noticed, for example, that ensemble methods like Bagging performed better overall, while simpler models like Naive Bayes worked quickly with minimal tuning. Although I didn't exactly learn in which cases I should use each model, it gave me the practical experience to begin recognising which ones might be more suitable depending on the goals — whether that's speed, simplicity, or accuracy.\n",
    "\n",
    "##### Student 2: ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
